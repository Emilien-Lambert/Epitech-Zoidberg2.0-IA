{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zoidberg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "metadata": {
      "interpreter": {
        "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7l0L_cHmU-I"
      },
      "source": [
        "> `Matthieu ALCARO, Emilien LAMBERT, Antonio VALLERA, Leo MARCEL, Fares QEDIRA`\n",
        "---\n",
        "\n",
        "# **ZOIDBERG 2.0**\n",
        "Dans ce cahier, nous allons utiliser 3 jeux de données d'images de radiographies de poumons pour aider les médecins à détecter si le patient a une pneumonie.\n",
        "\n",
        "Pour ce faire, nous allons d'abord classifier nos jeux de données car il s'agit de différents types d'images, certaines sont en R.G.B (3D) et d'autres en N&B (2D).\n",
        "Ensuite, nous les transformerons pour déterminer la maladie pulmonaire.\n",
        "\n",
        " En ce qui concerne notre travail précédent sur MNIST, qui consistait à reconnaître différents chiffres manuscrits, nous avons intégré notre logique MNIST dans ce projet 'zoidberg 2.0'.\n",
        "\n",
        "\n",
        "## 1. **Installation de paquets externes**\n",
        "\n",
        "Certains paquets et modules externes sont nécessaires pour faire fonctionner l'ensemble du module, comme :\n",
        "\n",
        ">- `matplotlib` *(Creates visualizations in Python)*\n",
        "    - `matimage` *(image loading, rescaling and display operations)*\n",
        "    - `pylot` *(functions to manipulate elements of a figure)*\n",
        "- `numpy` *(extended mathematical functions)*\n",
        "- `graphviz` *(create graph objects)*\n",
        "- `sklearn` *(provides learning algorithms)*\n",
        "    - `metrics`\n",
        "    - `decomposition`\n",
        "    - `kernel_approximation`\n",
        "    - `neural_network`\n",
        "    - `neighbors`\n",
        "    - `naive_bayes`\n",
        "    - `tree`\n",
        "    - `ensemble`\n",
        "    - `model_selection`\n",
        "- `math` *(basic math functions)*\n",
        "- `mnist` *(database of handwritten digits)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENR1GQ7tj-JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cd173d-c176-44f7-ec14-fe009eac7b86"
      },
      "source": [
        "#installation of dependencies\n",
        "!pip install sklearn matplotlib numpy graphviz python-mnist python-math image\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "#external packages\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import graphviz\n",
        "\n",
        "from sklearn import svm, metrics, tree\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from math import sqrt\n",
        "\n",
        "from mnist import MNIST\n",
        "\n",
        "from datetime import datetime, timedelta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Collecting python-mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f0/6086b84427c3bf156ec0b3c2f9dfc1d770b35f942b9ed8a64f5229776a80/python_mnist-0.7-py2.py3-none-any.whl\n",
            "Collecting python-math\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/8c/60c13be29a2f2e74c0313f2e62c7f751c944fe54b917afa5f88144e71a66/python_math-0.0.1-py3-none-any.whl\n",
            "Collecting image\n",
            "  Downloading https://files.pythonhosted.org/packages/84/be/961693ed384aa91bcc07525c90e3a34bc06c75f131655dfe21310234c933/image-1.5.33.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from image) (7.1.2)\n",
            "Collecting django\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/91/e23103dd21fa1b5c1fefb65c4d403107b10bf450ee6955621169fcc86db9/Django-3.2.2-py3-none-any.whl (7.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9MB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from image) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from django->image) (0.4.1)\n",
            "Collecting asgiref<4,>=3.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/17/8b/05e225d11154b8f5358e6a6d277679c9741ec0339d1e451c9cef687a9170/asgiref-3.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from django->image) (2018.9)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from asgiref<4,>=3.3.2->django->image) (3.7.4.3)\n",
            "Building wheels for collected packages: image\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=ef9069aa01ca0455c49e80092b4cdb600e19d04271aa0c8caa06dbb923ec808a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/4c/60/d5904e69c837fcdea7e03ffa0c657f35ced7e398c6f3ca17cc\n",
            "Successfully built image\n",
            "Installing collected packages: python-mnist, python-math, asgiref, django, image\n",
            "Successfully installed asgiref-3.3.4 django-3.2.2 image-1.5.33 python-math-0.0.1 python-mnist-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gY6DwUNspf"
      },
      "source": [
        "\n",
        "## *2*. **Fonction pour resize / formatter les images**\n",
        "\n",
        "#### Les images ne font pas toutes la même taille, de plus certaines sont en noir et blanc, d'autres en 3 dimensions (RGB). Nous devont donc les traiter afin de les utiliser avec différents modèles de ScikitLearn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmDZ3SV131hy"
      },
      "source": [
        "def crop(img, size):\n",
        "    middleH = img.shape[0] / 2\n",
        "    middleW = img.shape[1] / 2\n",
        "\n",
        "    lowH = middleH - (size / 2)\n",
        "    maxH = middleH + (size / 2)\n",
        "\n",
        "    lowW = middleW - (size / 2)\n",
        "    maxW = middleW + (size / 2)\n",
        "\n",
        "    cropped = img[int(lowH):int(maxH), int(lowW):int(maxW)]\n",
        "    return cropped\n",
        "\n",
        "def data_formatting(path):\n",
        "    # List all name of images file in a table\n",
        "    images_name = [f for f in os.listdir(path) if re.match(r'.*\\.jpeg', f)]\n",
        "\n",
        "    # Create table of label and image path\n",
        "    labels = []\n",
        "    images_path = []\n",
        "    counter = 0\n",
        "\n",
        "    # Create a table of reformatted images\n",
        "    images = []\n",
        "    good_images = []\n",
        "    bad_images = []\n",
        "\n",
        "    for image_name in images_name:\n",
        "        img = mpimg.imread(path + image_name)\n",
        "        if img.ndim == 2:\n",
        "            cropped = crop(img, 200)\n",
        "            cropped = cropped / 255\n",
        "            cropped = np.reshape(cropped, 40000)\n",
        "            good_images.append(cropped)\n",
        "            if \"virus\" in image_name:\n",
        "                labels.append(\"virus\")\n",
        "            elif \"bacteria\" in image_name:\n",
        "                labels.append(\"bacteria\")\n",
        "            else:\n",
        "                labels.append(\"normal\")\n",
        "\n",
        "    return good_images, labels\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIS4VCP4NzNn"
      },
      "source": [
        "## *3*. **Fonction pour charger les images (Mnist ou pneumonie selon le paramètre)**\n",
        "\n",
        "Data Formating permet de trier les data, et de renvoyer ainsi les images avec le bon label. On charge ensuite les images avec load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhiHI1UwS3XC"
      },
      "source": [
        "def data_formatting(path):\n",
        "    # List all name of images file in a table\n",
        "    images_name = [f for f in os.listdir(path) if re.match(r'.*\\.jpeg', f)]\n",
        "\n",
        "    # Create table of label and image path\n",
        "    labels = []\n",
        "    images_path = []\n",
        "    counter = 0\n",
        "\n",
        "    # Create a table of reformatted images\n",
        "    images = []\n",
        "    good_images = []\n",
        "    bad_images = []\n",
        "\n",
        "    for image_name in images_name:\n",
        "        img = mpimg.imread(path + image_name)\n",
        "        if img.ndim == 2:\n",
        "            cropped = crop(img, 200)\n",
        "            cropped = cropped / 255\n",
        "            cropped = np.reshape(cropped, 40000)\n",
        "            good_images.append(cropped)\n",
        "            if \"virus\" in image_name:\n",
        "                labels.append(\"virus\")\n",
        "            elif \"bacteria\" in image_name:\n",
        "                labels.append(\"bacteria\")\n",
        "            else:\n",
        "                labels.append(\"normal\")\n",
        "        elif img.ndim == 3:\n",
        "            bad_images.append(img)\n",
        "\n",
        "    return good_images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "macvFGdoN24Z"
      },
      "source": [
        "def load_dataset(type_data):\n",
        "    if type_data == 'mnist':\n",
        "        mn_data = MNIST('./mnist/')\n",
        "        images_training, labels_training = mn_data.load_training()\n",
        "        images_testing, labels_testing = mn_data.load_testing()\n",
        "        data = {\n",
        "            'np_images_training': np.array(images_training),\n",
        "            'np_labels_training': np.array(labels_training),\n",
        "            'np_images_testing': np.array(images_testing),\n",
        "            'np_labels_testing': np.array(labels_testing)\n",
        "        }\n",
        "        return data\n",
        "\n",
        "    elif type_data == 'pneumonia':\n",
        "\n",
        "        train_images, train_labels = data_formatting(\"data ia sorted/all/train/\")\n",
        "        test_images, test_labels = data_formatting(\"data ia sorted/all/test/\")\n",
        "        validation_images, validation_labels = data_formatting(\"data ia sorted/all/validation/\")\n",
        "        \n",
        "        data = {\n",
        "            'np_images_training': np.array(train_images),\n",
        "            'np_labels_training': np.array(train_labels),\n",
        "            'np_images_testing': np.array(test_images),\n",
        "            'np_labels_testing': np.array(test_labels),\n",
        "            \"np_images_validation\": np.array(validation_images),\n",
        "            \"np_labels_validation\": np.array(validation_labels),\n",
        "        }\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6stVhr80IuSU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 2. **Importation de jeux de données**\n",
        "\n",
        "Cette partie du code sert uniquement à importer le jeu de données dans notre code, jeu de données situé dans le dossier ./mnist et \"data ia sorted\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a6f_eLrL45ZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "41760907-2155-4eef-bcea-b7db645e0b7d"
      },
      "source": [
        "    print(\"--------------- START LOAD_DATASET ---------------\")\n",
        "    start_time = time.time()\n",
        "    data = load_dataset('pneumonia')\n",
        "    print(\"--------------- FINISH : %s SECONDS ---------------\" % (time.time() - start_time))\n",
        "    print(\"--------------- DATASET HAS BEEN IMPORTED ---------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- START LOAD_DATASET ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cc731e81b059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------- START LOAD_DATASET ---------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pneumonia'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------- FINISH : %s SECONDS ---------------\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------- DATASET HAS BEEN IMPORTED ---------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6b8f9304a2d3>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(type_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_data\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pneumonia'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_formatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data ia sorted/all/train/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_formatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data ia sorted/all/test/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mvalidation_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_formatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data ia sorted/all/validation/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c0c38956519a>\u001b[0m in \u001b[0;36mdata_formatting\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_formatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# List all name of images file in a table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimages_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*\\.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Create table of label and image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data ia sorted/all/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTSQNaYI5HPv"
      },
      "source": [
        "# 3. **Transform**\n",
        "\n",
        "Certain algo nécessite / et ou sont plus performant avec une action de type transform sur les images.\n",
        "La fonction Transform() permet de redimensionner les images afin qu'à la fin, elles est toutes les mêmes dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjVVwTr05Vwr"
      },
      "source": [
        "    # Les transform\n",
        "    print(\"--------------- START TRANSFORM ---------------\")\n",
        "    # start_time = time.time()\n",
        "    # transform = Transformation()\n",
        "    # train_data_transform = transform.nystroem(models.np_images_training)\n",
        "    # test_data_transform = transform.nystroem(models.np_images_testing)\n",
        "    # models.np_images_training = train_data_transform\n",
        "    # models.np_images_testing = test_data_transform\n",
        "    print(\"--------------- FINISH : %s SECONDS ---------------\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBBUXtVj55EN"
      },
      "source": [
        "# 4. **Algorithmes**\n",
        "\n",
        "Scikit-learn est une bibliothèque libre Python destinée à l'apprentissage automatique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APwFatxE6o3L"
      },
      "source": [
        "**On commence d'abord par initaliser les datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRg48wBN6nVh"
      },
      "source": [
        "def __init__(self, data):\n",
        "        self.np_images_training = data[\"np_images_training\"]\n",
        "        self.np_labels_training = data[\"np_labels_training\"]\n",
        "        self.np_images_testing = data[\"np_images_testing\"]\n",
        "        self.np_labels_testing = data[\"np_labels_testing\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9VBeFZP6yJR"
      },
      "source": [
        "**Méthode pour obtenir le score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNpTjcaX62KQ"
      },
      "source": [
        "def get_scores(self, model):\n",
        "        train_score = model.score(self.np_images_training, self.np_labels_training) * 100\n",
        "        test_score = model.score(self.np_images_testing, self.np_labels_testing) * 100\n",
        "        print('\\n\\n--- Training score : %.3f' % (train_score))\n",
        "        print('\\n--- Testing score : %.3f \\n\\n' % (test_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Z9Sm5069Hi"
      },
      "source": [
        "Une prédiction de classe est la suivante : étant donné le modèle finalisé et une ou plusieurs instances de données, prédire la classe pour les instances de données.\n",
        "Nous ne connaissons pas les classes de résultats pour les nouvelles données. C'est pourquoi nous avons besoin du modèle en premier lieu.\n",
        "Nous pouvons prédire la classe des nouvelles instances de données à l'aide de notre modèle de classification finalisé dans scikit-learn en utilisant la fonction predict().\n",
        "Par exemple, nos instances de données peut être transmis à la fonction predict() de notre modèle afin de prédire les valeurs de classe pour chaque instance du tableau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dc6j2Au6-JR"
      },
      "source": [
        "def get_predictions(self, model):\n",
        "        return model.predict(self.np_images_testing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gCD0mz7toM"
      },
      "source": [
        "Cette fonction va faire un tracé d'une matrice de confusion sklearn cm en utilisant une visualisation de carte thermique Seaborn. à montrer dans chaque carré. count : Si True, montre le nombre brut dans la matrice de confusion. La valeur par défaut est True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8b04Ptg7ugY"
      },
      "source": [
        "def get_matrix(self, model, predicted):\n",
        "        print(f\"Classification report for classifier {model}:\\n\"\n",
        "              f\"{metrics.classification_report(self.np_labels_testing, predicted)}\\n\")\n",
        "\n",
        "        disp = metrics.plot_confusion_matrix(model, self.np_images_testing, self.np_labels_testing)\n",
        "        disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "        print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEmtABLY8AHy"
      },
      "source": [
        "## SVC\n",
        "\n",
        "**Le SVC (Support Vector Classification) est un algorithme appartenant à l’ensemble des SVM (Support Vector Machines).** \n",
        "Le SVC prend plusieurs hyperparamètres, notamment le gamma.\n",
        "Plus le gamma est grand, plus l’algorithme essaie de s’adapter aux données de façon précise. On peut d’ailleurs constater qu’augmenter le gamma de façon trop conséquente conduit à un “overfitting” et donc un modèle moins précis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMujtopF8CUd"
      },
      "source": [
        "def svc(self):\n",
        "        clf = svm.SVC(verbose=True)\n",
        "        clf.fit(self.np_images_training, self.np_labels_training)\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHrcXnYK8IxB"
      },
      "source": [
        "Le SVC Linear est un SVC avec un paramètre kernel ‘Linear’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWiVZWji8MUW"
      },
      "source": [
        "def svc_linear(self):\n",
        "        clf = svm.LinearSVC(verbose=True)\n",
        "        clf.fit(self.np_images_training, self.np_labels_training)\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYH-_St48OrW"
      },
      "source": [
        "## MLP Classifier\n",
        "\n",
        "**Le MLP Classifier met en œuvre un algorithme de perceptron multicouche (MLP) qui s'entraîne par rétro propagation.**\n",
        "MLP s'entraîne sur deux tableaux : le tableau X de taille (n_samples, n_features), qui contient les échantillons d'entraînement représentés sous forme de vecteurs de caractéristiques en virgule flottante ; et le tableau y de taille (n_samples,) qui contient les valeurs cibles (étiquettes de classe) pour les échantillons d'entraînement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8H45w_Q8YrP"
      },
      "source": [
        "def mlp_classifier(self):\n",
        "        clf = MLPClassifier(verbose=True, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(784, 3), random_state=1)\n",
        "        clf.fit(self.np_images_training, self.np_labels_training)\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfOe78GS8eJw"
      },
      "source": [
        "## Prédiction KNN \n",
        "\n",
        "**Nearest Neighbors met en œuvre l'apprentissage non supervisé des plus proches voisins.** Le choix de l'algorithme de recherche des voisins est contrôlé par le mot clé 'algorithm', qui doit être l'un de ['auto', 'ball_tree', 'kd_tree', 'brute']. Lorsque la valeur par défaut \"auto\" est utilisée, l'algorithme tente de déterminer la meilleure approche à partir des données d'apprentissage.\n",
        "\n",
        "Pour calculer la valeur la plus adaptée du k (le nombre de voisins utilisé par l’algorithme), il faut prendre la racine carré du nombre total d’image d’entrainement (train). Dans notre cas, on laisse le choix de l’algorithme a la valeur par défaut pour utiliser le plus adapté. Cela permet d'entraîner au mieux possible l’algorithme et d’avoir une bonne précision lors de la phase de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvp8MvQx8nZH"
      },
      "source": [
        "    def prediction_knn(self):\n",
        "        # Create KNN Classifier\n",
        "        k = round(sqrt(self.np_labels_training.size))  # k does be square root of the training set records\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)  # call the k nearest neighbors\n",
        "\n",
        "        # Train the model using the training sets\n",
        "        knn.fit(self.np_images_training, self.np_labels_training)\n",
        "\n",
        "        # Predict the response for test dataset\n",
        "        predicted = knn.predict(self.np_images_testing)\n",
        "\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8GZbxla8r7R"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "**Les méthodes Naive Bayes sont un ensemble d'algorithmes d'apprentissage supervisé basés sur l'application du théorème de Bayes.** Le théorème de Bayes établit une relation prenant en compte la variable de classe et un vecteur de caractéristiques. \n",
        "Les classificateurs Naive Bayes ont très bien fonctionné dans de nombreuses situations du monde réel, notamment dans la classification des documents et le filtrage. Ils nécessitent une petite quantité de données d'entraînement pour estimer les paramètres nécessaires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMSiXdrz8uRF"
      },
      "source": [
        "def naive_bayes(self):\n",
        "        model = GaussianNB()\n",
        "        # fit the model with the training data\n",
        "        model.fit(self.np_images_training, self.np_labels_training)\n",
        "        \n",
        "        predicted = model.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEQKdHDE86fs"
      },
      "source": [
        "## Decision Tree Classifier\n",
        "\n",
        "**DecisionTreeClassifier est une classe capable d'effectuer une classification multi-classes sur un ensemble de données.**\n",
        "Comme les autres classificateurs, Decision Tree Classifier prend en entrée deux tableaux : un tableau d’entrainement, clairsemé ou dense, de forme (images_training, labels_training) contenant les échantillons d'apprentissage, et un tableau de test de valeurs entières, de forme (images_tests, labels_tests), contenant les étiquettes de classe pour les échantillons d'apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjAmHrMz9EEz"
      },
      "source": [
        "def decision_tree_classifier(self, max_depth):\n",
        "        # max_depth = nombre de niveau dans l'arbre (+ grand = + precis (jusqu'a un certain point), - graph lisible)\n",
        "        # entropy = par rapport au gain\n",
        "        clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, min_samples_split=2, random_state=0)\n",
        "        clf = clf.fit(self.np_images_training, self.np_labels_training)\n",
        "\n",
        "        # scores = cross_val_score(clf, self.np_images_training, self.np_labels_training, cv=5)\n",
        "        # print(scores.mean())\n",
        "\n",
        "        # export to pdf the training tree classification schema\n",
        "        dot_data = export_graphviz(clf, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(\"Tree_Graph\")\n",
        "\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ_qeXTZ9HhR"
      },
      "source": [
        "## Random Tree forest\n",
        "\n",
        "**Random Forest est un modèle d'ensemble composé de nombreux arbres de décision,\n",
        "des sous-ensembles aléatoires de caractéristiques et le vote moyen pour faire des prédictions.**\n",
        "\n",
        "Le fonctionnement de l’algorithme est donc similaire à celui du Decision Tree Classifier avec en paramètre supplémentaire (n_estimators) qui correspond au nombre d'arbres qu’il va exécuter. Plus ce paramètre sera grand plus les performances seront élevées mais le code sera ralenti.\n",
        "\n",
        "Pour récupérer un graphique en résultat, on récupère un graphique aléatoire parmi les différents Decision Tree Classifier effectuer par le Random Tree Forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETVghHT9Ilp"
      },
      "source": [
        "def random_tree_forest(self, estimators, max_depth):\n",
        "        # n_estimators = + le nombre est grand, + les performances seront bonnes mais le code sera ralenti\n",
        "        clf = RandomForestClassifier(n_estimators=estimators, max_depth=max_depth, min_samples_split=2, random_state=0)\n",
        "        clf = clf.fit(self.np_images_training, self.np_labels_training)\n",
        "\n",
        "        # scores = cross_val_score(clf, self.np_images_training, self.np_labels_training, cv=5)\n",
        "        # print(scores.mean())\n",
        "\n",
        "        # create graph in a pdf\n",
        "        # take a random tree in the forest and display it !!!\n",
        "        estimator = clf.estimators_[randrange(estimators)]\n",
        "        dot_data = export_graphviz(estimator, out_file=None, filled=True, rounded=True, special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(\"Tree_Forest_Graph\")\n",
        "\n",
        "        # Predict the response for test dataset\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ge4F07N9MfX"
      },
      "source": [
        "## Extremely Randomized Trees\n",
        "\n",
        "**Le classificateur ExtraTrees (Extremely Randomized Trees) fonctionne à l'identique par rapport au Random Tree Forest.** Extra Trees teste des divisions aléatoires sur une fraction des caractéristiques (contrairement à Random Forest, qui teste toutes les divisions possibles sur une fraction des caractéristiques)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3QWtE6F9Njb"
      },
      "source": [
        "def extremely_randomized_trees(self, estimators, max_depth):\n",
        "        # ExtraTrees classifier always tests random splits over fraction of features\n",
        "        # (in contrast to RandomForest, which tests all possible splits over fraction of features)\n",
        "\n",
        "        clf = ExtraTreesClassifier(n_estimators=estimators, max_depth=max_depth, min_samples_split=2, random_state=0)\n",
        "        clf = clf.fit(self.np_images_training, self.np_labels_training)\n",
        "\n",
        "        # Predict the response for test dataset\n",
        "        predicted = clf.predict(self.np_images_testing)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(self.np_labels_testing, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om9TXZzU-S1F"
      },
      "source": [
        "# 6. **Exécution**\n",
        "\n",
        "Dans le main on va pouvoir selectionner le type d'algo que l'on souhaite exécuter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ldXcN5-cJ6"
      },
      "source": [
        "def main():\n",
        "    print(\"--------------- START LOAD_DATASET ---------------\")\n",
        "    start_time = time.time()\n",
        "    data = load_dataset('pneumonia')\n",
        "    print(\"--------------- FINISH : %s SECONDS ---------------\" % (time.time() - start_time))\n",
        "\n",
        "    models = Sklearn(data)\n",
        "\n",
        "    # visualize = Statistics()\n",
        "    # visualize.pca_3d(models.np_images_training, models.np_labels_training)\n",
        "\n",
        "    # Les transform\n",
        "    print(\"--------------- START TRANSFORM ---------------\")\n",
        "    # start_time = time.time()\n",
        "    # transform = Transformation()\n",
        "    # train_data_transform = transform.nystroem(models.np_images_training)\n",
        "    # test_data_transform = transform.nystroem(models.np_images_testing)\n",
        "    # models.np_images_training = train_data_transform\n",
        "    # models.np_images_testing = test_data_transform\n",
        "    print(\"--------------- FINISH : %s SECONDS ---------------\" % (time.time() - start_time))\n",
        "\n",
        "    # Les algo\n",
        "    print(\"--------------- START TRAINING ---------------\")\n",
        "    start_time = time.time()\n",
        "    # models.svc()\n",
        "    # print(models.svc_linear())\n",
        "    # my_model = models.mlp_classifier()\n",
        "\n",
        "    # models.prediction_knn()\n",
        "    # models.naive_bayes()\n",
        "    # models.decision_tree_classifier(5)\n",
        "    models.random_tree_forest(100, 5)\n",
        "    # models.extremely_randomized_trees(100, 10)\n",
        "\n",
        "    print(\"--------------- FINISH : %s SECONDS ---------------\" % (time.time() - start_time))\n",
        "\n",
        "    # Les stats\n",
        "    # models.get_scores(my_model)\n",
        "    # predictions = models.get_predictions(my_model)\n",
        "    # models.get_matrix(my_model, predictions)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}